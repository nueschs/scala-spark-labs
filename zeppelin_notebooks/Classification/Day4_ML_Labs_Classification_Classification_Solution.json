{"paragraphs":[{"text":"%md\n\n## Sentiment Analysis with Twitter Data\n\nIn this lab, we are analyzing a big chunk of twitter data. The goal is to build a classifier that can decide if a tweet is positive or negative. \n\nTo not occupy you the whole day with just getting the data into a clean shape, we already did a little prework for you: \nWe removed some special things like *@mentions*, did lower-case normalization and put the data into a CSV file that is easily readable.\n\nAlso, the data is labeled – means that every tweet is categorized as\n    0 – negative tweet\n    1 – positive tweet\n\nSneak peek at the data:\n    id,text,target \n    ...\n    3,my whole body feels itchy and like its on fire,0\n    4,no it s not behaving at all i m mad why am i here because i can t see you all over there,0\n    ...\n    1599990,wooooo xbox is back,1\n    1599991,mmmm that sounds absolutely perfect but my schedule is full i won t have time to lay in bed until sunday ugh,1\n    ...\n\nThe data is stored under */user/zeppelin/clean_tweet.csv* – have fun!\n","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Sentiment Analysis with Twitter Data</h2>\n<p>In this lab, we are analyzing a big chunk of twitter data. The goal is to build a classifier that can decide if a tweet is positive or negative.</p>\n<p>To not occupy you the whole day with just getting the data into a clean shape, we already did a little prework for you:\n<br  />We removed some special things like <em>@mentions</em>, did lower-case normalization and put the data into a CSV file that is easily readable.</p>\n<p>Also, the data is labeled – means that every tweet is categorized as</p>\n<pre><code>0 – negative tweet\n1 – positive tweet\n</code></pre>\n<p>Sneak peek at the data:</p>\n<pre><code>id,text,target \n...\n3,my whole body feels itchy and like its on fire,0\n4,no it s not behaving at all i m mad why am i here because i can t see you all over there,0\n...\n1599990,wooooo xbox is back,1\n1599991,mmmm that sounds absolutely perfect but my schedule is full i won t have time to lay in bed until sunday ugh,1\n...\n</code></pre>\n<p>The data is stored under <em>/user/zeppelin/clean_tweet.csv</em> – have fun!</p>\n"}]},"apps":[],"jobName":"paragraph_1550157676617_-762271880","id":"20190212-071613_1674421940","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:239"},{"text":"%md\n### Classification Pipeline Basics\n\nA classification pipeline should roughly include the following steps:\n1. Read in data\n2. Clean/normalize data (if needed – mostly already done for you here)\n3. Split data into training and test set and continue only with the *TRAINING set*\n4. Extract features from the data*\n5. Learn a classifier based on your features\n6. Evaluate the classifiers performance on the *TEST set*, e.g. by calculating recall & precision\n\n\n#### *Note on step 4 (feature design)\nThe features to design here can be arbitrarily complex. As we discussed in the course, it is a good idea to start **as simple as possible**, which we will do in the code below with the following steps:\n- First step: Just randomly predict positive/negative to have a baseline (= the absolute minimum your model should achieve)\n- Second step: Look for some keywords in the text which could indicate positive or negative feelings\n- Third step: Use the words of the tweet, converted to features\n- Fourth step: Extract more advanced feature like TF-IDF (frequency-inverse document frequency)\n\n\n**Let's dive into it!**","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Classification Pipeline Basics</h3>\n<p>A classification pipeline should roughly include the following steps:</p>\n<ol>\n<li>Read in data</li>\n<li>Clean/normalize data (if needed – mostly already done for you here)</li>\n<li>Split data into training and test set and continue only with the <em>TRAINING set</em></li>\n<li>Extract features from the data*</li>\n<li>Learn a classifier based on your features</li>\n<li>Evaluate the classifiers performance on the <em>TEST set</em>, e.g. by calculating recall &amp; precision</li>\n</ol>\n<h4>*Note on step 4 (feature design)</h4>\n<p>The features to design here can be arbitrarily complex. As we discussed in the course, it is a good idea to start <strong>as simple as possible</strong>, which we will do in the code below with the following steps:</p>\n<ul>\n<li>First step: Just randomly predict positive/negative to have a baseline (= the absolute minimum your model should achieve)</li>\n<li>Second step: Look for some keywords in the text which could indicate positive or negative feelings</li>\n<li>Third step: Use the words of the tweet, converted to features</li>\n<li>Fourth step: Extract more advanced feature like TF-IDF (frequency-inverse document frequency)</li>\n</ul>\n<p><strong>Let's dive into it!</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1550157676617_697108854","id":"20190212-074007_744718913","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"title":"Read In Data and Normalize","text":"// load the data\nval df = (spark\n    .read\n    .option(\"inferSchema\", true) // needed so that the label (0, 1) is read as integer, not string\n    .option(\"header\", true) // csv has headers\n    .option(\"delim\", \",\") // delimiter: ,\n    .csv(\"/user/zeppelin/clean_tweet.csv\")) // file location\n\n// convert to DataFrame which has two columns:\n// \"text\" (String) \n// \"label\" (Double) \n\n// note: the label should be of type Double to work correctly with logistic regression later\n\nval dfCleaned = (df\n    .na.drop // drops lines with non-available values (not necessary with our clean data) \n    .select($\"text\", $\"target\".alias(\"label\").cast(\"Double\")))\n","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676617_-881716216","id":"20190209-164608_1353438803","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:241"},{"title":"Split Data into Training and Test","text":"// split the data into a training and test set\n// a commonly used proportion: 80% training / 20% test (but feel free to play around with that!)\n\nval Array(trainingData, testData) = dfCleaned.randomSplit(Array(0.8, 0.2))","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676617_529075969","id":"20190209-164521_1811653806","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"title":"Random Baseline","text":"// to have a baseline that your algorithms below should beat, let's try random guessing\n// for this, add a column to your test dataframe with the randomly guessed labels\n\nval testWithRandomLabels = testData.withColumn(\"randomLabel\", when(rand() > 0.5, 1d).otherwise(0d))\n","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676618_-1294183660","id":"20190212-191434_1854884194","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:243"},{"title":"Printing Metrics (Precision, Recall, Accuracy)","text":"// to measure how good the baseline is and how good our algorithms below will do, let's define a function here\n// that we can also reuse below\n// the function should take a DataFrame as a parameter and then print out precision, recall and accuracy \n\nimport org.apache.spark.sql.DataFrame\n\ndef printPredictionMetrics(df:DataFrame, predictedLabelsColumn:String, trueLabelsColumn:String) = {\n    val TP = df.filter(s\"$trueLabelsColumn == 1.0 AND $predictedLabelsColumn == 1.0\").count().toDouble // true positive rate\n    val FP = df.filter(s\"$trueLabelsColumn == 0.0 AND $predictedLabelsColumn == 1.0\").count().toDouble // false positive rate\n    val TN = df.filter(s\"$trueLabelsColumn == 0.0 AND $predictedLabelsColumn == 0.0\").count().toDouble // true negative rate\n    val FN = df.filter(s\"$trueLabelsColumn == 1.0 AND $predictedLabelsColumn == 0.0\").count().toDouble // false negative rate\n    \n    println(\"Precision: \" + TP/(TP+FP))\n    println(\"Recall: \" + TP/(TP+FN))\n    println(\"Accuracy: \" + (TP+TN)/(TP+FP+TN+FN))\n\n}\n\n// test the function with your randomly guessed labels here. do the results make sense?\nprintPredictionMetrics(testWithRandomLabels, \"randomLabel\", \"label\")\n\n\n// NOTE: the code skeleton/solution to calculate the metrics we provide here is not super optimal from a computational point of view.\n// there might be more sophisticated solutions  - e.g. we developed one with groupByKey & flatMapGroups, but it is much harder to understand for a Scala beginner.\n\n// if you're interested, have a look at the last paragraph at the bottom of this notebook\n// or maybe you find an even better one yourself (let us know)! :-)","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676618_-860632000","id":"20190212-201756_1651157369","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:244"},{"title":"Simple Algorithm: Look for Keywords in Text","text":"// as a next step, let's try to manually define some words that might indicate positive or negative feelings \n\n// a possible approach would be: if you find more positive than negative words in a text, assume a positive tweet (label 1)\n// if there are more negative words, assume a negative one (label 0)\n\n// definition of a map that weighs words as either positive or negative\nval heuristicMap = Map(\n    \"happy\" -> 1,\n    \"positive\" -> 1,\n    \"hooray\" -> 1,\n    \"nice\" -> 1,\n    \"great\" -> 1,\n    \"beautiful\" -> 1,\n    \"hooray\" -> 1,\n    \"love\" -> 1,\n    \"peace\" -> 1,\n    \"sad\" -> -1,\n    \"angry\" -> -1,\n    \"shit\" -> -1,\n    \"negative\" -> -1,\n    \"dumb\" -> -1\n)\n\n// initialize a random number generator to guess a label if you do not find any positive or negative words in a tweet\nimport scala.util.Random\nval r = new Random(System.currentTimeMillis)\n\n// define a function that:\n// - takes a text as input\n// - splits the text into words\n// - maps each words against the dictionary defined above\n// - sums up the overall score and sets it to 1 (if positive wins), 0 (if negative wins) or random guesses if no side wins\ndef containsHeuristicEntry(text:String):Double = (text\n  .split(\"\\\\s\")\n  .map(word => heuristicMap.getOrElse(word, 0))\n  .sum match {\n    case i if i > 0 => 1d // if the positive words win: set label 1\n    case i if i < 0 => 0d // if the negative words win: set label 0\n    case _ => r.nextInt(2) // if there is a draw or there are no words from our dictionary: random guess 0 or 1\n  })\n  \n// to apply the function defined above to a DataFrame column, you need to wrap it inside a UDF\nval heuristicUDF = udf[Double,String](containsHeuristicEntry)\n\n// finally: add a column to your test DataFrame which contains a guess on the labels based on your function you defined above\nval testWithKeywordAlgorithm = testWithRandomLabels.withColumn(\"keywordLabel\", heuristicUDF($\"text\"))\n\n// check the prediction metrics with your simple algorithm: is it better than the random baseline? (if not, your code might not work the way you want)\nprintPredictionMetrics(testWithKeywordAlgorithm, \"keywordLabel\", \"label\")\n","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676618_-1310166807","id":"20190212-161742_1274952317","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:245"},{"title":"Simple ML Pipeline: Extract Word Features and Train Logistic Regression","text":"// let's build a simple ML pipeline now:\n// - tokenize the text into single words\n// - calculate the term frequency of the words (term frequency = \"#occurences of a word within a text\")\n// - use the term frequencies as features to train a logistic regression\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer, IDF}\n\n// start with a tokenizing step to split your text column into single words\nval tokenizer = (new Tokenizer()\n    .setInputCol(\"text\")\n    .setOutputCol(\"words\"))\n  \n// then add a step that calculates term frequency with a hashing function based on your word column --> these term frequencies will be your features\nval hashingTF = (new HashingTF()\n    .setNumFeatures(1000)\n    .setInputCol(\"words\")\n    .setOutputCol(\"features\"))\n\n// add a logistic regression step\nval lr = (new LogisticRegression()\n    .setMaxIter(1000)\n    .setRegParam(0.001))\n  \n// put together the three steps above into a ML pipeline\nval pipeline = new Pipeline()\n    .setStages(Array(tokenizer, hashingTF, lr))\n\n// train your pipeline\nval model = pipeline.fit(trainingData)\n\n// transform the data you want to measure your accuracy on\nval predictions = model.transform(testData) \n\n// check the prediction accuracy of your ML model: is it better than the algorithms before?\nprintPredictionMetrics(predictions, \"prediction\", \"label\")\n","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676618_2126607865","id":"20190210-063809_1192386446","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:246"},{"text":"%md\n#### Optional pipeline improvement: use the TF-IDF statistic as feature\n\n**If you have enough time, you can also try the following improvement:**\nThe so called TF-IDF (**T**erm **F**requency multiplied by **I**nverse **D**ocument **F**requency) is a very popular feature in ML models that work with documents.\nIt is a better measure than the raw term frequency we used above, in the sense that it tries to put an importance measure on words:\n- If the word occurs in almost every document, it is not really a \"unique feature\" and hence the weight of that word is put to a small value (e.g. \"the\", \"a\", \"that\").\n- On the other hand, if a word is very rare, it seems to be an important word and it gets a high weight (e.g. \"frustrating\", \"disappointing\", \"enlight\").\n\nTo improve your model, convert your term frequency (the result of the HashingTF step) into TF-IDF by just adding one single step to your pipeline:\n```\nval idf = new IDF()\n   .setInputCol([output column of your hashingTF])\n   .setOutputCol([final feature column])\n```\n\nThen change your pipeline to also include the *idf* step between *hashingTF* and *lr* and see if you can get a better score!\n\n*If you want to read up more on TF-IDF, have a look at its' [Wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or just google for \"TF-IDF\" and you will find lots of resources.*","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Optional pipeline improvement: use the TF-IDF statistic as feature</h4>\n<p><strong>If you have enough time, you can also try the following improvement:</strong>\n<br  />The so called TF-IDF (<strong>T</strong>erm <strong>F</strong>requency multiplied by <strong>I</strong>nverse <strong>D</strong>ocument <strong>F</strong>requency) is a very popular feature in ML models that work with documents.\n<br  />It is a better measure than the raw term frequency we used above, in the sense that it tries to put an importance measure on words:</p>\n<ul>\n<li>If the word occurs in almost every document, it is not really a &ldquo;unique feature&rdquo; and hence the weight of that word is put to a small value (e.g. &ldquo;the&rdquo;, &ldquo;a&rdquo;, &ldquo;that&rdquo;).</li>\n<li>On the other hand, if a word is very rare, it seems to be an important word and it gets a high weight (e.g. &ldquo;frustrating&rdquo;, &ldquo;disappointing&rdquo;, &ldquo;enlight&rdquo;).</li>\n</ul>\n<p>To improve your model, convert your term frequency (the result of the HashingTF step) into TF-IDF by just adding one single step to your pipeline:</p>\n<pre><code>val idf = new IDF()\n   .setInputCol([output column of your hashingTF])\n   .setOutputCol([final feature column])\n</code></pre>\n<p>Then change your pipeline to also include the <em>idf</em> step between <em>hashingTF</em> and <em>lr</em> and see if you can get a better score!</p>\n<p><em>If you want to read up more on TF-IDF, have a look at its' <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Wikipedia page</a> or just google for &ldquo;TF-IDF&rdquo; and you will find lots of resources.</em></p>\n"}]},"apps":[],"jobName":"paragraph_1550157676618_-871114473","id":"20190212-220336_195481549","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"title":"Appendix: More Sophisticated Function to Calculate Precision Metrics","text":"// the following function calculates accuracy, recall and precision in a more sophisticated and Scala/FP way!\n// have a look at it and ask us if there is any pattern that you completely don't understand.\n\ndef printMetricsForLabel(df:DataFrame, pred:String, label:String):Unit = {\n  val preds = df.select(col(pred),col(label)).as[(Double,Double)]\n\n  val precisionPerLabel = preds.groupByKey(_._1).flatMapGroups( \n    (k, iter) => {\n       val (correct, count) = iter.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n       List( k -> (correct / count))\n    }\n  ).toDF(\"label\", \"precision\")\n\n  val recallPerLabel = preds.groupByKey(_._2).flatMapGroups( \n    (k, iter) => {\n       val (correct, count) = iter.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n       List( k -> (correct / count))\n    }\n  ).toDF(\"label\", \"recall\")\n\n  val (correct, count) = preds.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n  \n  precisionPerLabel.show()\n  recallPerLabel.show()\n  println(s\"Overall Accuracy: $correct / $count = ${correct / count}\")\n}\n\n\nprintln(\"Random Performance\")\nprintMetricsForLabel(testWithWordClassification, \"randomlabel\", \"label\")\n\nprintln(\"Heuristic Performance\")\nprintMetricsForLabel(testWithWordClassification, \"keywordLabel\", \"label\")\n\nprintln(\"Logistic Regression Performance\")\nprintMetricsForLabel(testWithWordClassification, \"wordClassificationLabel\", \"label\")","user":"admin","dateUpdated":"2019-02-14T15:21:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550157676618_-730292923","id":"20190210-064345_637863997","dateCreated":"2019-02-14T15:21:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:248"}],"name":"Day4_ML_Labs/Classification/Classification_Solution","id":"2E6ETH75S","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}