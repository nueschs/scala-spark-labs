{"paragraphs":[{"text":"%md\n\n## Sentiment Analysis with Twitter Data\n\nIn this lab, we are analyzing a big chunk of twitter data. The goal is to build a classifier that can decide if a tweet is positive or negative. \n\nTo not occupy you the whole day with just getting the data into a clean shape, we already did a little prework for you: \nWe removed some special things like *@mentions*, did lower-case normalization and put the data into a CSV file that is easily readable.\n\nAlso, the data is labeled – means that every tweet is categorized as\n    0 – negative tweet\n    1 – positive tweet\n\nSneak peek at the data:\n    id,text,target \n    ...\n    3,my whole body feels itchy and like its on fire,0\n    4,no it s not behaving at all i m mad why am i here because i can t see you all over there,0\n    ...\n    1599990,wooooo xbox is back,1\n    1599991,mmmm that sounds absolutely perfect but my schedule is full i won t have time to lay in bed until sunday ugh,1\n    ...\n\nThe data is stored under */user/zeppelin/clean_tweet.csv* – have fun!\n","user":"admin","dateUpdated":"2019-02-12T22:39:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Sentiment Analysis with Twitter Data</h2>\n<p>In this lab, we are analyzing a big chunk of twitter data. The goal is to build a classifier that can decide if a tweet is positive or negative.</p>\n<p>To not occupy you the whole day with just getting the data into a clean shape, we already did a little prework for you:\n<br  />We removed some special things like <em>@mentions</em>, did lower-case normalization and put the data into a CSV file that is easily readable.</p>\n<p>Also, the data is labeled – means that every tweet is categorized as</p>\n<pre><code>0 – negative tweet\n1 – positive tweet\n</code></pre>\n<p>Sneak peek at the data:</p>\n<pre><code>id,text,target \n...\n3,my whole body feels itchy and like its on fire,0\n4,no it s not behaving at all i m mad why am i here because i can t see you all over there,0\n...\n1599990,wooooo xbox is back,1\n1599991,mmmm that sounds absolutely perfect but my schedule is full i won t have time to lay in bed until sunday ugh,1\n...\n</code></pre>\n<p>The data is stored under <em>/user/zeppelin/clean_tweet.csv</em> – have fun!</p>\n"}]},"apps":[],"jobName":"paragraph_1550011167933_-1556413387","id":"20190212-071613_1674421940","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4997"},{"text":"%md\n### Classification Pipeline Basics\n\nA classification pipeline should roughly include the following steps:\n1. Read in data\n2. Clean/normalize data (if needed – mostly already done for you here)\n3. Split data into training and test set and continue only with the *TRAINING set*\n4. Extract features from the data*\n5. Learn a classifier based on your features\n6. Evaluate the classifiers performance on the *TEST set*, e.g. by calculating recall & precision\n\n\n#### *Note on step 4 (feature design)\nThe features to design here can be arbitrarily complex. As we discussed in the course, it is a good idea to start **as simple as possible**, which we will do in the code below with the following steps:\n- First step: Just randomly predict positive/negative to have a baseline (= the absolute minimum your model should achieve)\n- Second step: Look for some keywords in the text which could indicate positive or negative feelings\n- Third step: Use the words of the tweet, converted to features\n- Fourth step: Extract more advanced feature like TF-IDF (frequency-inverse document frequency)\n\n\n**Let's dive into it!**","user":"admin","dateUpdated":"2019-02-12T22:39:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Classification Pipeline Basics</h3>\n<p>A classification pipeline should roughly include the following steps:</p>\n<ol>\n<li>Read in data</li>\n<li>Clean/normalize data (if needed – mostly already done for you here)</li>\n<li>Split data into training and test set and continue only with the <em>TRAINING set</em></li>\n<li>Extract features from the data*</li>\n<li>Learn a classifier based on your features</li>\n<li>Evaluate the classifiers performance on the <em>TEST set</em>, e.g. by calculating recall &amp; precision</li>\n</ol>\n<h4>*Note on step 4 (feature design)</h4>\n<p>The features to design here can be arbitrarily complex. As we discussed in the course, it is a good idea to start <strong>as simple as possible</strong>, which we will do in the code below with the following steps:</p>\n<ul>\n<li>First step: Just randomly predict positive/negative to have a baseline (= the absolute minimum your model should achieve)</li>\n<li>Second step: Look for some keywords in the text which could indicate positive or negative feelings</li>\n<li>Third step: Use the words of the tweet, converted to features</li>\n<li>Fourth step: Extract more advanced feature like TF-IDF (frequency-inverse document frequency)</li>\n</ul>\n<p><strong>Let's dive into it!</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1550011167933_1440290988","id":"20190212-074007_744718913","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4998"},{"title":"Read In Data and Normalize","text":"// load the data\nval df = (spark\n    .read\n    .option(\"inferSchema\", true) // needed so that the label (0, 1) is read as integer, not string\n    .option(\"header\", ???) // TODO: is there a header?\n    .option(\"delim\", ???) // TODO: what's the delimiter of your CSV?\n    .csv(\"/user/zeppelin/clean_tweet.csv\")) // file location\n\n\n// TODO: convert the raw DataFrame read in above to one that has two columns:\n// \"text\" (String) \n// \"label\" (Double) \n// (note: the label should be of type Double to work correctly with logistic regression later)\n\nval dfCleaned = (df\n    .select($\"???\", $\"???\".alias(???).cast(???)))\n","user":"admin","dateUpdated":"2019-02-12T22:41:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167934_-1716953022","id":"20190209-164608_1353438803","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4999"},{"title":"Split Data into Training and Test","text":"// TODO: randomly split the data into a training and test set\n// a commonly used proportion: 80% training / 20% test (but feel free to play around with that!)\n\n// hint: Spark provides a very convenient function to split a DataFrame!\n\nval Array(trainingData, testData) = dfCleaned.???","user":"admin","dateUpdated":"2019-02-12T22:42:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167934_1768231644","id":"20190209-164521_1811653806","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5000"},{"title":"Random Baseline","text":"// TODO: to have a baseline that your algorithms below should beat, let's try random guessing.\n// for this, add a column to your test dataframe that is called \"randomLabel\" and randomly either has a 0 or 1\n// note that the type of your randomLabel should also be Double like the true label!\n\n// hint: the rand() function from Spark that creates a column from a uniform distribution between 0 and 1 might be very useful!\n\nval testWithRandomLabels = testData.withColumn(\"randomLabel\", when(??? > ???, ???).otherwise(???))\n","user":"admin","dateUpdated":"2019-02-12T22:47:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167934_1566503579","id":"20190212-191434_1854884194","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5001"},{"title":"Printing Metrics (Precision, Recall, Accuracy)","text":"// to measure how good the baseline is and how good our algorithms below will do, let's define a function here\n// that we can also reuse below\n// the function should take a DataFrame as a parameter and then print out precision, recall and accuracy \n\nimport org.apache.spark.sql.DataFrame\n\ndef printPredictionMetrics(df:DataFrame, predictedLabelsColumn:String, trueLabelsColumn:String) = {\n    \n    val TP = df.??? // TODO: calculate true positive rate\n    val FP = df.??? // TODO: calculate false positive rate\n    val TN = df.??? // TODO: calculate true negative rate\n    val FN = df.??? // TODO: calculate false negative rate\n    \n    println(\"Precision: \" + ???)) // TODO: calculate precision\n    println(\"Recall: \" + ???)) // TODO: calculate recall\n    println(\"Accuracy: \" + ???) // TODO: calculate accuracy\n\n}\n\n// test the function with your randomly guessed labels here. do the results make sense?\nprintPredictionMetrics(testWithRandomLabels, \"randomLabel\", \"label\")\n\n\n// NOTE: the code skeleton/solution to calculate the metrics we provide here is not super optimal from a computational point of view.\n// there might be more sophisticated solutions  - e.g. we developed one with groupByKey & flatMapGroups, but it is much harder to understand for a Scala beginner.\n\n// if you're interested, have a look at the last paragraph at the bottom of this notebook\n// or maybe you find an even better one yourself (let us know)! :-)","user":"admin","dateUpdated":"2019-02-12T22:50:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167934_1339176935","id":"20190212-201756_1651157369","dateCreated":"2019-02-12T22:39:27+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5002","dateFinished":"2019-02-12T22:49:14+0000","dateStarted":"2019-02-12T22:49:14+0000"},{"title":"Simple Algorithm: Look for Keywords in Text","text":"// as a next step, let's try to manually define some words that might indicate positive or negative feelings \n\n// a possible approach would be: if you find more positive than negative words in a text, assume a positive tweet (label 1)\n// if there are more negative words, assume a negative one (label 0)\n\n// definition of a map that weighs words as either positive or negative\nval heuristicMap = Map( \n    ??? -> 1,\n    ??? -> 1,\n    ??? -> -1,\n    ??? // TODO: complete the map with as many as you'd like\n)\n\n// initialize a random number generator to guess a label if you do not find any positive or negative words in a tweet\nimport scala.util.Random\nval r = new Random(System.currentTimeMillis)\n\n// define a function that:\n// - takes a text as input\n// - splits the text into words\n// - maps each words against the dictionary defined above\n// - sums up the overall score and sets it to 1 (if positive wins), 0 (if negative wins) or random guesses if no side wins\n\ndef containsHeuristicEntry(text:String):Double = (text\n  .split(???) // TODO: by what should you split a text string to get the individual words?\n  .map(word => heuristicMap.getOrElse(word, 0))\n  .sum match {\n    ??? // TODO: if the positive words win: return a Double 1\n    ??? // TODO: if the negative words win: return a Double 0\n    case _ => r.??? // TODO: use the random number generator to randomly guess a 0 or 1, if neither positive or negative is stronger\n  })\n  \n// to apply the function defined above to a DataFrame column, you need to wrap it inside a UDF\nval heuristicUDF = udf[Double,String](containsHeuristicEntry)\n\n// finally: add a column to your test DataFrame which contains a guess on the labels based on your function you defined above\nval testWithKeywordAlgorithm = testWithRandomLabels.withColumn(\"keywordLabel\", heuristicUDF($\"text\"))\n\n\n??? // TODO: use the function that you defined above to measure the accuracy of the simple algorithm you just developed!\n// are the results better than the random baseline? (if not, there might be an error in your code)\n","user":"admin","dateUpdated":"2019-02-12T22:55:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167935_-1430287933","id":"20190212-161742_1274952317","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5003"},{"title":"Simple ML Pipeline: Extract Word Features and Train Logistic Regression","text":"// let's build a simple ML pipeline now:\n// - tokenize the text into single words\n// - calculate the term frequency of the words (term frequency = \"#occurences of a word within a text\")\n// - use the term frequencies as features to train a logistic regression\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer, IDF}\n\n// start with a tokenizing step to split your text column into single words\nval tokenizer = (new Tokenizer()\n    .setInputCol(???)\n    .setOutputCol(???)) // TODO: complete the tokenizer transformer (hint: which column do you want to transform into which one?)\n    \n  \n// then add a step that calculates term frequency with a hashing function based on your word column --> these term frequencies will be your features\nval hashingTF = (new HashingTF()\n    .setNumFeatures(1000) // this sets the number of bins you hash into - play with the parameter to see if it affects the accuracy!\n    .???) // TODO: complete the HashingTF transformer (hint: it needs an input and an output column like the transformer before)\n\n\n// add a logistic regression step\nval lr = (new LogisticRegression()\n    .setMaxIter(1000)\n    .setRegParam(0.001)) // TODO: as you learned in the course, this value could play a crucial role in your final prediction accuracy! be sure to try different ones\n  \n// put together the three steps above into a ML pipeline\nval pipeline = new Pipeline()\n    .setStages(Array(???)) // TODO: what do you put in your pipeline?\n\n// TODO: train your pipeline with training data\nval model = pipeline.fit(???)\n\n// TODO: predict for your test data to measure accuracy\nval predictions = model.transform(???) \n\n// TODO: check the prediction accuracy of your ML model: is it better than the algorithms before?\nprintPredictionMetrics(???)\n","user":"admin","dateUpdated":"2019-02-12T23:01:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167935_-509087223","id":"20190210-063809_1192386446","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5004"},{"text":"%md\n#### Optional pipeline improvement: use the TF-IDF statistic as feature\n\n**If you have enough time, you can also try the following improvement:**\nThe so called TF-IDF (**T**erm **F**requency multiplied by **I**nverse **D**ocument **F**requency) is a very popular feature in ML models that work with documents.\nIt is a better measure than the raw term frequency we used above, in the sense that it tries to put an importance measure on words:\n- If the word occurs in almost every document, it is not really a \"unique feature\" and hence the weight of that word is put to a small value (e.g. \"the\", \"a\", \"that\").\n- On the other hand, if a word is very rare, it seems to be an important word and it gets a high weight (e.g. \"frustrating\", \"disappointing\", \"enlight\").\n\nTo improve your model, convert your term frequency (the result of the HashingTF step) into TF-IDF by just adding one single step to your pipeline:\n```\nval idf = new IDF()\n   .setInputCol([output column of your hashingTF])\n   .setOutputCol([final feature column])\n```\n\nThen change your pipeline to also include the *idf* step between *hashingTF* and *lr* and see if you can get a better score!\n\n*If you want to read up more on TF-IDF, have a look at its' [Wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or just google for \"TF-IDF\" and you will find lots of resources.*","user":"admin","dateUpdated":"2019-02-12T22:39:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Optional pipeline improvement: use the TF-IDF statistic as feature</h4>\n<p><strong>If you have enough time, you can also try the following improvement:</strong>\n<br  />The so called TF-IDF (<strong>T</strong>erm <strong>F</strong>requency multiplied by <strong>I</strong>nverse <strong>D</strong>ocument <strong>F</strong>requency) is a very popular feature in ML models that work with documents.\n<br  />It is a better measure than the raw term frequency we used above, in the sense that it tries to put an importance measure on words:</p>\n<ul>\n<li>If the word occurs in almost every document, it is not really a &ldquo;unique feature&rdquo; and hence the weight of that word is put to a small value (e.g. &ldquo;the&rdquo;, &ldquo;a&rdquo;, &ldquo;that&rdquo;).</li>\n<li>On the other hand, if a word is very rare, it seems to be an important word and it gets a high weight (e.g. &ldquo;frustrating&rdquo;, &ldquo;disappointing&rdquo;, &ldquo;enlight&rdquo;).</li>\n</ul>\n<p>To improve your model, convert your term frequency (the result of the HashingTF step) into TF-IDF by just adding one single step to your pipeline:</p>\n<pre><code>val idf = new IDF()\n   .setInputCol([output column of your hashingTF])\n   .setOutputCol([final feature column])\n</code></pre>\n<p>Then change your pipeline to also include the <em>idf</em> step between <em>hashingTF</em> and <em>lr</em> and see if you can get a better score!</p>\n<p><em>If you want to read up more on TF-IDF, have a look at its' <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Wikipedia page</a> or just google for &ldquo;TF-IDF&rdquo; and you will find lots of resources.</em></p>\n"}]},"apps":[],"jobName":"paragraph_1550011167935_136027644","id":"20190212-220336_195481549","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5005"},{"title":"Appendix: More Sophisticated Function to Calculate Precision Metrics","text":"// the following function calculates accuracy, recall and precision in a more sophisticated and Scala/FP way!\n// have a look at it and ask us if there is any pattern that you completely don't understand.\n\ndef printMetricsForLabel(df:DataFrame, pred:String, label:String):Unit = {\n  val preds = df.select(col(pred),col(label)).as[(Double,Double)]\n\n  val precisionPerLabel = preds.groupByKey(_._1).flatMapGroups( \n    (k, iter) => {\n       val (correct, count) = iter.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n       List( k -> (correct / count))\n    }\n  ).toDF(\"label\", \"precision\")\n\n  val recallPerLabel = preds.groupByKey(_._2).flatMapGroups( \n    (k, iter) => {\n       val (correct, count) = iter.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n       List( k -> (correct / count))\n    }\n  ).toDF(\"label\", \"recall\")\n\n  val (correct, count) = preds.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n  \n  precisionPerLabel.show()\n  recallPerLabel.show()\n  println(s\"Overall Accuracy: $correct / $count = ${correct / count}\")\n}\n\n\nprintln(\"Random Performance\")\nprintMetricsForLabel(testWithWordClassification, \"randomlabel\", \"label\")\n\nprintln(\"Heuristic Performance\")\nprintMetricsForLabel(testWithWordClassification, \"keywordLabel\", \"label\")\n\nprintln(\"Logistic Regression Performance\")\nprintMetricsForLabel(testWithWordClassification, \"wordClassificationLabel\", \"label\")","user":"admin","dateUpdated":"2019-02-12T22:39:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1550011167935_-549614468","id":"20190210-064345_637863997","dateCreated":"2019-02-12T22:39:27+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5006"}],"name":"Day4_ML_Labs/Classification/Classification_Medium","id":"2E3Q3UX62","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}