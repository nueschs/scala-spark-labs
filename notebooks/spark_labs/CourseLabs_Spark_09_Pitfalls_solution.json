{"paragraphs":[{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549721958742_1425654189","id":"20190209-141918_1278929889","dateCreated":"2019-02-09T14:19:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:952","text":"//note that this is not serializable. just like a real service endpoint wouldn't be\nimport scalikejdbc._\n\nclass MyTotallyLegitService(val hostName:String){\n    Class.forName(\"org.postgresql.Driver\")\n    ConnectionPool.singleton(\"jdbc:postgresql://10.0.10.24:5432/training\", \"trainer\", \"trainingpass\")\n    \n    implicit val session = AutoSession\n    \n    sql\"\"\"\n    CREATE TABLE IF NOT EXISTS side_effect (entry varchar)\n    \"\"\".execute().apply()\n    \n\n    \n    def translateGrade(grade:String):Int ={\n        grade match {\n            case \"A++\" => hostName.length + 5\n            case \"A+\" => hostName.length + 4\n            case \"A\" => hostName.length + 3\n            case \"A-\" => hostName.length + 2\n            case \"B\" => hostName.length + 1\n            case _ => hostName.length\n        }\n    }\n    \n    def appendToFile(msg:String):Unit = sql\"insert into side_effect (entry) values (${msg})\".update.apply\n    \n    def state:Int  = sql\"select count(*) from side_effect\".map(_.int(1)).single.apply().getOrElse(0)\n    \n}","dateUpdated":"2019-02-09T15:21:34+0000","dateFinished":"2019-02-09T15:21:34+0000","dateStarted":"2019-02-09T15:21:34+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import scalikejdbc._\ndefined class MyTotallyLegitService\n"}]}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549722037084_-388651571","id":"20190209-142037_1441821577","dateCreated":"2019-02-09T14:20:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1021","text":"import spark.implicits._\nval filename = \"/user/zeppelin/data.csv\"\nval df = spark.read.option(\"header\", true).option(\"inferSchema\", false).csv(filename)\n","dateUpdated":"2019-02-09T15:20:18+0000","dateFinished":"2019-02-09T15:20:26+0000","dateStarted":"2019-02-09T15:20:18+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nfilename: String = /user/zeppelin/data.csv\ndf: org.apache.spark.sql.DataFrame = [Rank: string, Grade: string ... 4 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-0-10-24.us-west-2.compute.internal:4040/jobs/job?id=0"],"interpreterSettingId":"spark2"}}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549722136412_-728379476","id":"20190209-142216_1119615542","dateCreated":"2019-02-09T14:22:16+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1163","text":"val serviceInstance = new MyTotallyLegitService(\"SuperHost\")\ndf.map(\n    row => {\n        val channel = row.getAs[String](\"Channel name\")\n        channel -> serviceInstance.translateGrade(channel)\n    }\n).show","dateUpdated":"2019-02-09T14:36:52+0000","dateFinished":"2019-02-09T14:35:26+0000","dateStarted":"2019-02-09T14:35:25+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Task not serializable\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345)\n  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n  at org.apache.spark.SparkContext.clean(SparkContext.scala:2299)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:844)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:843)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:843)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:608)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)\n  ... 55 elided\nCaused by: java.io.NotSerializableException: MyTotallyLegitService\nSerialization stack:\n\t- object not serializable (class: MyTotallyLegitService, value: MyTotallyLegitService@3af92cca)\n\t- field (class: $iw, name: serviceInstance, type: class MyTotallyLegitService)\n\t- object (class $iw, $iw@70c85290)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@59f22662)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@1fe84374)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@13ada8ee)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@7a1c76bf)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@409d29c9)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@4d8fdac7)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@699e7722)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@2601e37e)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@7edcfea0)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@70f98619)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@63130949)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@2ef805d4)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@3f8809bd)\n\t- field (class: $line74978272134.$read, name: $iw, type: class $iw)\n\t- object (class $line74978272134.$read, $line74978272134.$read@6e989166)\n\t- field (class: $iw, name: $line74978272134$read, type: class $line74978272134.$read)\n\t- object (class $iw, $iw@4c9a6d58)\n\t- field (class: $iw, name: $outer, type: class $iw)\n\t- object (class $iw, $iw@1be8fb24)\n\t- field (class: $anonfun$1, name: $outer, type: class $iw)\n\t- object (class $anonfun$1, <function1>)\n\t- element of array (index: 2)\n\t- array (class [Ljava.lang.Object;, size 5)\n\t- field (class: org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, name: references$1, type: class [Ljava.lang.Object;)\n\t- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, <function2>)\n  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)\n  ... 86 more\n"}]}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549722915271_1717025835","id":"20190209-143515_177389982","dateCreated":"2019-02-09T14:35:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1872","text":"\ndf.mapPartitions(\n    partition => {\n        val serviceInstance = new MyTotallyLegitService(\"SuperHost\")\n        partition.map(\n            row => {\n                val channel = row.getAs[String](\"Channel name\")\n                channel -> serviceInstance.translateGrade(channel)\n            }\n        )\n    }\n).show","dateUpdated":"2019-02-09T15:05:33+0000","dateFinished":"2019-02-09T15:05:33+0000","dateStarted":"2019-02-09T15:05:33+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+---+\n|                  _1| _2|\n+--------------------+---+\n|              Zee TV|  9|\n|            T-Series|  9|\n|Cocomelon - Nurse...|  9|\n|           SET India|  9|\n|                 WWE|  9|\n|          Movieclips|  9|\n|          netd müzik|  9|\n|ABS-CBN Entertain...|  9|\n|     Ryan ToysReview|  9|\n|         Zee Marathi|  9|\n|     5-Minute Crafts|  9|\n|     Canal KondZilla|  9|\n|    Like Nastya Vlog|  9|\n|               Ozuna|  9|\n|          Wave Music|  9|\n|         Ch3Thailand|  9|\n|     WORLDSTARHIPHOP|  9|\n|     Vlad and Nikita|  9|\n|             Badabun|  9|\n|   WorkpointOfficial|  9|\n+--------------------+---+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-0-10-24.us-west-2.compute.internal:4040/jobs/job?id=19"],"interpreterSettingId":"spark2"}}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549722992940_-672294157","id":"20190209-143632_372293257","dateCreated":"2019-02-09T14:36:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1993","text":"val mapped = df.mapPartitions(\n    partition => {\n        val serviceInstance = new MyTotallyLegitService(\"SuperHost\")\n        partition.map(\n            row => {\n                val channel = row.getAs[String](\"Channel name\")\n                serviceInstance.appendToFile(channel)\n                channel\n            }\n        )\n    }\n)","dateUpdated":"2019-02-09T15:25:20+0000","dateFinished":"2019-02-09T15:25:21+0000","dateStarted":"2019-02-09T15:25:20+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mapped: org.apache.spark.sql.Dataset[String] = [value: string]\n"}]}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549723338018_-408211244","id":"20190209-144218_422510897","dateCreated":"2019-02-09T14:42:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2224","text":"val serviceInstance = new MyTotallyLegitService(\"SuperHost\")\nmapped.map(s => s + \"a\").foreach(_ => ())\nprintln(serviceInstance.state)\nmapped.map(s => s + \"a\").foreach(_ => ())\nprintln(serviceInstance.state)\nval storedCached = mapped.map(s => s + \"a\").cache\nstoredCached.map(s => s + \"a\").foreach(_ => ())\nprintln(serviceInstance.state)\nstoredCached.map(s => s + \"a\").foreach(_ => ())\nprintln(serviceInstance.state)","dateUpdated":"2019-02-09T15:26:02+0000","dateFinished":"2019-02-09T15:26:16+0000","dateStarted":"2019-02-09T15:26:02+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"30066\n35066\n40066\n40066\nserviceInstance: MyTotallyLegitService = MyTotallyLegitService@cb7239c\nstoredCached: org.apache.spark.sql.Dataset[String] = [value: string]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-10-0-10-24.us-west-2.compute.internal:4040/jobs/job?id=26","http://ip-10-0-10-24.us-west-2.compute.internal:4040/jobs/job?id=27","http://ip-10-0-10-24.us-west-2.compute.internal:4040/jobs/job?id=28","http://ip-10-0-10-24.us-west-2.compute.internal:4040/jobs/job?id=29"],"interpreterSettingId":"spark2"}}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549723719526_-2131476302","id":"20190209-144839_1832108804","dateCreated":"2019-02-09T14:48:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2441","text":"","dateUpdated":"2019-02-09T15:22:21+0000","dateFinished":"2019-02-09T15:22:08+0000","dateStarted":"2019-02-09T15:22:08+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"5022\nserviceInstance: MyTotallyLegitService = MyTotallyLegitService@15d042c3\n"}]}},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549724669481_-402042850","id":"20190209-150429_843441447","dateCreated":"2019-02-09T15:04:29+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3488"}],"name":"CourseLabs/Spark/09_Pitfalls_solution","id":"2E3CN4BKE","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}