{"paragraphs":[{"text":"//This should cover all the Imports you'll need to get this done. If you get something along the Lines of \"Symbol not Found\" ask your instructor they probably know the appropriate import\nimport scala.util.{Try, Success, Failure}\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame","user":"admin","dateUpdated":"2019-02-09T12:36:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549715667530_-870834262","id":"20190209-123427_2014828464","dateCreated":"2019-02-09T12:34:27+0000","dateStarted":"2019-02-09T12:36:14+0000","dateFinished":"2019-02-09T12:36:15+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:79"},{"text":"val filename = \"/user/zeppelin/data.csv\"\n//Read in the data here. You should set schema-inference to disabled and enable headers\nval df:DataFrame = ???","user":"admin","dateUpdated":"2019-02-09T12:37:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549715764371_1055642630","id":"20190209-123604_655692257","dateCreated":"2019-02-09T12:36:04+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"text":"//if you're interested in some quick and dirty stats about your columns. you'll see that the numeric columns aren't all that numeric\ndf.describe(df.columns:_*).show","user":"admin","dateUpdated":"2019-02-09T12:38:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549715831350_360323492","id":"20190209-123711_544991217","dateCreated":"2019-02-09T12:37:11+0000","dateStarted":"2019-02-09T12:37:22+0000","dateFinished":"2019-02-09T12:37:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"text":"//Do your analyis of the columns here to figure out how you have to filter the dataset later to throw out the useless data\nval toBeRejected:DataFrame = df.filter(???)","user":"admin","dateUpdated":"2019-02-09T12:38:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549715842869_-254761196","id":"20190209-123722_534047161","dateCreated":"2019-02-09T12:37:22+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"text":"//Time to convert this into a Dataset so we have a higher degree of certainty about our datas structure\n\nfinal case class VideoEntry(???)\n\nval ds:Dataset[VideoEntry] = df\n    .filter(???) //throw out useless data\n    .map(\n        row => Try {\n            //do conversion. You'll have to handle the clean up of the Rank Column here as well\n            //row.getAs let's you access columns by name and type. in our case all columns are of type string\n            row.getAs[String](\"Channel name\")\n            \n            VideoEntry(???)\n        } match {\n            case Success(entry) => entry // If its all a success we just pass the entry along\n            case Failure(err) => throw err //if it wasn't we crash. This would mean we haven't sufficiently filtered our dataframe or that our transformation logic is faults\n        }\n        //Indeed this is a bit a verbose way to do it with this kind of Try. However normally you might have recovery logic in case of failure or at least log.\n    )\n    .as[VideoEntry] //this line will actually transform the DataFrame to a Dataset[VideoEntry]\n    \nds.createOrReplaceTempView(\"Videos\")","user":"admin","dateUpdated":"2019-02-09T12:46:43+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549715914094_-909530262","id":"20190209-123834_373551790","dateCreated":"2019-02-09T12:38:34+0000","dateStarted":"2019-02-09T12:43:21+0000","dateFinished":"2019-02-09T12:43:21+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"text":"%sql\n--You can do your Querries here to the \"Videos\" table","user":"admin","dateUpdated":"2019-02-09T12:45:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549716201394_-1841897716","id":"20190209-124321_1839813545","dateCreated":"2019-02-09T12:43:21+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"}],"name":"CourseLabs/Spark/08_Skeleton","id":"2E4HKFC8W","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}